from sqlalchemy.orm import Session
from sqlalchemy import func, case, desc, and_, or_
from app.models.inventory import Inventory
from app.models.returns import Return
from app.models.stores import Store
from app.models.remediation_recommendations import RemediationRecommendation
from app.models.return_remediation import ReturnRemediation
from app.utils.dashboard_filters import apply_inventory_filters
from typing import Optional, Dict, List
from datetime import date, datetime, timedelta
import pandas as pd
import numpy as np


# ------------------ 1. Filter Function ------------------ #
def apply_filters(df, filters):
    df_filtered = df.copy()

    if filters.get('store_channel'):
        df_filtered = df_filtered[df_filtered["store_channel"] == filters['store_channel']]
    if filters.get('store_id'):
        df_filtered = df_filtered[df_filtered["store_id"] == filters['store_id']]
    if filters.get('region'):
        df_filtered = df_filtered[df_filtered["region"] == filters['region']]
    
    # Always apply time_period filter (defaults to today if not provided)
    df_filtered["date"] = pd.to_datetime(df_filtered["date"], errors="coerce")
    df_filtered = df_filtered[df_filtered["date"] <= pd.to_datetime(filters['time_period'])]

    return df_filtered


# ------------------ 2. Impact Tracker Calculation ------------------ #
def impact_tracker_calculation(remediation_df, return_remediation_df, selected_date):
    # Ensure proper datetime
    remediation_df["date"] = pd.to_datetime(remediation_df["received_date"])
    return_remediation_df["date"] = pd.to_datetime(return_remediation_df["return_date"])
    selected_date = pd.to_datetime(selected_date)

    # Merge both
    all_df = pd.concat([remediation_df, return_remediation_df], ignore_index=True)

    # --------------- KPI FUNCTIONS --------------- #
    def todays_loss():
        value = all_df.loc[all_df["date"] == selected_date, "net_loss_mitigation"].sum()
        return value

    def mtd_loss():
        value = all_df.loc[
            (all_df["date"].dt.month == selected_date.month) &
            (all_df["date"].dt.year == selected_date.year),
            "net_loss_mitigation"
        ].sum()
        return value

    def ytd_loss():
        value = all_df.loc[
            all_df["date"].dt.year == selected_date.year,
            "net_loss_mitigation"
        ].sum()
        return value

    def shrinkage_alerts():
        value = all_df.loc[all_df["date"] == selected_date].shape[0]
        return value

    def incident_frequency():
        store_alerts = remediation_df.loc[remediation_df["date"] == selected_date].shape[0]
        return_alerts = return_remediation_df.loc[return_remediation_df["date"] == selected_date].shape[0]

        if store_alerts > return_alerts:
            value = "Store"
        elif return_alerts > store_alerts:
            value = "Return"
        else:
            value = "Equal"
        return value

    def last_7_days_loss():
        last_7_days = all_df.loc[
            (all_df["date"] <= selected_date) &
            (all_df["date"] > selected_date - pd.Timedelta(days=7))
        ]

        value = (
            last_7_days.groupby("date")["net_loss_mitigation"].sum()
            .reindex(pd.date_range(selected_date - pd.Timedelta(days=6), selected_date), fill_value=0)
            .reset_index()
            .rename(columns={"index": "date", "net_loss_mitigation": "loss"})
            .to_dict(orient="records")
        )
        return value

    def monthly_store_loss():
        value = (
            all_df.loc[
                (all_df["date"].dt.month == selected_date.month) &
                (all_df["date"].dt.year == selected_date.year)
            ]
            .groupby("store_id")["net_loss_mitigation"].sum()
            .nlargest(10)
            .reset_index()
            .to_dict(orient="records")
        )
        return value

    # --------------- Final Return --------------- #
    return {
        "todays_loss_mitigation": float(todays_loss()),
        "month_to_date_loss_mitigation": float(mtd_loss()),
        "year_to_date_loss_mitigation": float(ytd_loss()),
        "shrinkage_alerts_triggered": int(shrinkage_alerts()),
        "incident_frequency": incident_frequency(),
        "last_7_days_loss_mitigation": last_7_days_loss(),
        "monthly_loss_mitigation_per_store_top10": monthly_store_loss(),
    }


def get_impact_tracker_summary(
    db: Session,
    time_period: Optional[str] = None,
    region: Optional[str] = None,
    store_id: Optional[str] = None,
    store_channel: Optional[str] = None
) -> Dict:
    """
    Get comprehensive impact tracker summary with all KPIs
    """
    try:
        # Set default time_period to today if not provided
        if not time_period:
            time_period = datetime.now().strftime('%Y-%m-%d')
        
        # Get data from database
        remediation_data = db.query(RemediationRecommendation).all()
        return_remediation_data = db.query(ReturnRemediation).all()
        
        # Convert to DataFrames
        remediation_df = pd.DataFrame([{
            'store_id': item.store_id,
            'category': item.category,
            'received_date': item.received_date,
            'net_loss_mitigation': item.net_loss_mitigation,
            'issue_id': item.issue_id,
            'risk_level': item.risk_level
        } for item in remediation_data])
        
        return_remediation_df = pd.DataFrame([{
            'store_id': item.store_id,
            'category': item.category,
            'return_date': item.return_date,
            'net_loss_mitigation': item.net_loss_mitigation,
            'issue_id': item.issue_id
        } for item in return_remediation_data])
        
        # Process data similar to your logic
        if not remediation_df.empty:
            # Filter out LOW and CRITICAL risk levels, keep only HIGH and VERY_HIGH
            remediation_df = remediation_df[~remediation_df['risk_level'].isin(['LOW', 'CRITICAL'])]
            
            # For VERY_HIGH risk, keep only max net_loss_mitigation per group
            if 'VERY_HIGH' in remediation_df['risk_level'].values:
                mask = (
                    (remediation_df["risk_level"] != "VERY_HIGH") |
                    (
                        remediation_df["net_loss_mitigation"] ==
                        remediation_df.groupby(["store_id", "issue_id"])["net_loss_mitigation"].transform("max")
                    )
                )
                remediation_df = remediation_df[mask]
        
        if not return_remediation_df.empty:
            # Filter out null issue_ids
            return_remediation_df = return_remediation_df[return_remediation_df['issue_id'].notnull()]
        
        # Apply filters
        filters = {}
        if region:
            filters['region'] = region
        if store_id:
            filters['store_id'] = store_id
        if store_channel:
            filters['store_channel'] = store_channel
        
        # Always include time_period filter (defaults to today)
        filters['time_period'] = time_period
        
        # Apply filters to both DataFrames
        remediation_df = apply_filters(remediation_df, filters)
        return_remediation_df = apply_filters(return_remediation_df, filters)
        
        # Calculate KPIs using time_period as the selected_date
        kpis = impact_tracker_calculation(remediation_df, return_remediation_df, time_period)
        
        # Only include filters that were actually provided
        applied_filters = {}
        if region:
            applied_filters["region"] = region
        if store_id:
            applied_filters["store_id"] = store_id
        if time_period:
            applied_filters["time_period"] = time_period
        if store_channel:
            applied_filters["store_channel"] = store_channel
        
        return {
            "success": True,
            "data": kpis,
            "filters_applied": applied_filters
        }
    except Exception as e:
        return {
            "success": False,
            "error": str(e),
            "data": None
        }












